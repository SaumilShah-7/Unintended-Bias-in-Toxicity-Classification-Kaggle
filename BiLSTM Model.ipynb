{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Fasttext + Glove (Unintended Bias).ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaumilShah-7/Unintended-Bias-in-Toxicity-Classification-Kaggle/blob/master/BiLSTM%20Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "86eeab46-ed9d-4ce8-9666-9f979a036970",
        "_cell_guid": "00d73702-f1e7-4ee8-b1e2-b8f6180aa8e4",
        "trusted": true,
        "id": "LhL2-Wqk7ybP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import time\n",
        "from tqdm.notebook import tqdm_notebook as tqdm\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "import regex as re\n",
        "import copy\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import gc\n",
        "\n",
        "max_features = 100000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d82b969a-30b0-4096-8e91-38261081f751",
        "_cell_guid": "36876fad-cda3-448e-89a5-bd9bc377d598",
        "trusted": true,
        "id": "1goLiHuu7yba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
        "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
        "\n",
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "5df2577a-0f1e-4828-bd11-c244482dfb77",
        "_cell_guid": "9e699cee-3c03-4f23-aedf-7b918b8632a0",
        "trusted": true,
        "id": "PLNhgSiH7ybj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install Unidecode\n",
        "from unidecode import unidecode\n",
        "\n",
        "words_only = re.compile(r'[^A-Za-z\\']')\n",
        "def clean_text(x):\n",
        "    x_ascii = unidecode(x)\n",
        "    x_clean = words_only.sub(' ', x_ascii)\n",
        "    return x_clean\n",
        "\n",
        "train['clean_text'] = train['comment_text'].apply(lambda x: clean_text(x))\n",
        "test['clean_text'] = test['comment_text'].apply(lambda x: clean_text(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "27ea52b7-2afb-4306-8e59-e08c63fbe239",
        "_cell_guid": "553efb2c-4a08-4176-8e7c-10a6009eccef",
        "trusted": true,
        "id": "gMTLCJgA7ybp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train['comment_text'][0])\n",
        "print(train['clean_text'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "29cc6255-a471-4d43-a7ed-80fb9bd95bfa",
        "_cell_guid": "6017833a-c8a4-4cb7-8eba-65e283f9fd9a",
        "trusted": true,
        "id": "IFp1mSoa7ybv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = text.Tokenizer(num_words=max_features)\n",
        "t.fit_on_texts(list(train['clean_text'])+list(test['clean_text']))\n",
        "\n",
        "print(len(t.word_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "2a85ecd8-65b2-47f2-9654-8a0335a124e9",
        "_cell_guid": "1d606420-6c34-471b-84ca-15d063641762",
        "trusted": true,
        "id": "OjjGU9fV7ybz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = t.word_index\n",
        "word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e0cbf37a-86e8-4f1e-96e0-c9fbf4a43894",
        "_cell_guid": "3d9a5811-b26a-4ce9-bf96-d485c4ce7ecd",
        "trusted": true,
        "id": "nVzppcfZ7yb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = t.texts_to_sequences(train['clean_text'])\n",
        "X_test = t.texts_to_sequences(test['clean_text'])\n",
        "\n",
        "l = list(map(len, X_train))\n",
        "print('Min: %d, Mean: %d, Q3: %d, Max: %d' %(min(l), sum(l)/len(l), np.percentile(l, 75), max(l)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4f975bdd-1076-478f-a244-d94063e91e53",
        "_cell_guid": "d66c3625-b99c-4de9-ba91-e3d0bd0240d9",
        "trusted": true,
        "id": "sovc2Cre7yb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 220\n",
        "\n",
        "x_train = sequence.pad_sequences(X_train, maxlen=MAX_LEN)\n",
        "x_test = sequence.pad_sequences(X_test, maxlen=MAX_LEN)\n",
        "\n",
        "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
        "\n",
        "aux_columns = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'sexual_explicit']\n",
        "y_aux_train = train[aux_columns].fillna(0)\n",
        "\n",
        "print(x_train.shape, x_test.shape, y_train.shape, y_aux_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "39b2e6e1-e761-4a4d-92c9-ce14959803c5",
        "_cell_guid": "fbf143dd-6171-4130-8bfc-99f19f646d6a",
        "trusted": true,
        "id": "9-w4PK627yb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "identity_columns = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish', 'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
        "\n",
        "weights = np.ones((len(train),)) / 4\n",
        "\n",
        "# Subgroup\n",
        "weights += (train[identity_columns].fillna(0).values >= 0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n",
        "\n",
        "# Background Positive, Subgroup Negative\n",
        "weights += (((train['target'].values >= 0.5).astype(np.int) + (train[identity_columns].fillna(0).values < 0.5).sum(axis=1).astype(bool).astype(np.int)) > 1).astype(bool).astype(np.int) / 4\n",
        "\n",
        "# Background Negative, Subgroup Positive\n",
        "weights += (((train['target'].values < 0.5).astype(np.int) + (train[identity_columns].fillna(0).values >= 0.5).sum(axis=1).astype(bool).astype(np.int)) > 1).astype(bool).astype(np.int) / 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "214e389a-1371-4c30-8593-03f38f29872c",
        "_cell_guid": "ea03dc3d-54f5-4888-a481-7f0833b949b8",
        "trusted": true,
        "id": "SdmvY3Gf7ycC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('x_train.npy', x_train)\n",
        "np.save('x_test.npy', x_test)\n",
        "np.save('y_train.npy', y_train)\n",
        "np.save('y_aux_train.npy', y_aux_train)\n",
        "np.save('weights.npy', weights)\n",
        "\n",
        "with open('word_index.pickle', 'wb') as handle:\n",
        "  pickle.dump(word_index, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "5c7841a5-3385-4141-99b4-5f0d2c194455",
        "_cell_guid": "7b665247-75b4-4865-9157-1eb5465767d5",
        "trusted": true,
        "_kg_hide-output": false,
        "id": "aiXkfV4j7ycF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del train, X_train, X_test, x_train, x_test, y_train, y_aux_train, t, word_index, weights\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "20a69cae-44cf-491b-bebc-2a714f29118a",
        "_cell_guid": "bba6c0f2-7a0b-44f4-a8ef-729f86b40801",
        "trusted": true,
        "id": "l64XYQvL7ycJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ft_path = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
        "gl_path = '../input/glove840b300dtxt/glove.840B.300d.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "a680cacf-9017-4e7d-a805-c90ebe16b801",
        "_cell_guid": "fd8b4357-a40a-4620-bff0-01956c5655bc",
        "trusted": true,
        "id": "4geOjdHz7ycN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import gensim\n",
        "# model = gensim.models.KeyedVectors.load_word2vec_format(ft_path)\n",
        "\n",
        "# words = model.index2word\n",
        "\n",
        "# w_rank = {}\n",
        "# for i,word in enumerate(words):\n",
        "#     w_rank[word] = i\n",
        "\n",
        "# WORDS = w_rank\n",
        "\n",
        "# del model, words, w_rank\n",
        "# gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f3920aba-99f2-4b83-a102-929cf2fc5108",
        "_cell_guid": "de9ca84d-0117-4126-8ac1-f136b3807126",
        "trusted": true,
        "id": "JvPGMTyK7ycR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "# def P(word): \n",
        "#     \"Probability of `word`.\"\n",
        "#     # use inverse of rank as proxy\n",
        "#     # returns 0 if the word isn't in the dictionary\n",
        "#     return - WORDS.get(word, 0)\n",
        "\n",
        "# def correction(word): \n",
        "#     \"Most probable spelling correction for word.\"\n",
        "#     return max(candidates(word), key=P)\n",
        "\n",
        "# def candidates(word): \n",
        "#     \"Generate possible spelling corrections for word.\"\n",
        "#     return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "# def known(words): \n",
        "#     \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "#     return set(w for w in words if w in WORDS)\n",
        "\n",
        "# def edits1(word):\n",
        "#     \"All edits that are one edit away from `word`.\"\n",
        "#     letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "#     splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "#     deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "#     transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "#     replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "#     inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "#     return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "# def edits2(word): \n",
        "#     \"All edits that are two edits away from `word`.\"\n",
        "#     return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "62a59ed8-e337-486c-8adb-55ce755f4651",
        "_cell_guid": "359957b2-c964-4c09-89cc-55c8e7e6d6e5",
        "trusted": true,
        "scrolled": true,
        "id": "cEv6pOyY7ycU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_coefs(word,*arr):\n",
        "  return word, np.asarray(arr, dtype='float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "a9bc2ba2-bea3-4a33-a2a5-3243a3bb7366",
        "_cell_guid": "bec627cc-242b-4617-b256-c8d74b2025cd",
        "trusted": true,
        "id": "N1ZaLI1B7ycZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with open('word_index.pickle', 'rb') as handle:\n",
        "#     word_index = pickle.load(handle)\n",
        "\n",
        "# nb_words = min(max_features, len(word_index))\n",
        "# word_process = re.compile(r'[^A-Za-z]')\n",
        "\n",
        "# def getword(embeddings_keys, word):\n",
        "#     if word in embeddings_keys:\n",
        "#         return word\n",
        "#     elif word.lower() in embeddings_keys:\n",
        "#         return word.lower()\n",
        "#     elif word.upper() in embeddings_keys:\n",
        "#         return word.upper()\n",
        "#     elif word.capitalize() in embeddings_keys:\n",
        "#         return word.capitalize()\n",
        "#     elif word_process.sub('', word) in embeddings_keys:\n",
        "#         return word_process.sub('', word)\n",
        "#     elif len(word)>1 and len(word)<=15:\n",
        "#         x = correction(word)\n",
        "#         if x in embeddings_keys:\n",
        "#             return x\n",
        "\n",
        "#     return None\n",
        "\n",
        "# def build_matrix(path, nb_words, embed_size):\n",
        "#     embeddings = dict(get_coefs(*o.strip().split(' ')) for o in open(path))\n",
        "#     embeddings_keys = list(embeddings.keys())\n",
        "#     corrected = []\n",
        "#     words_not_found = []\n",
        "#     matrix = np.zeros((nb_words, embed_size))\n",
        "    \n",
        "#     for word, i in tqdm(word_index.items()):\n",
        "#         if i >= nb_words:\n",
        "#             break\n",
        "#         else:\n",
        "#             word2 = getword(embeddings_keys, word)\n",
        "#             if word2 is not None:\n",
        "#                 matrix[i] = embeddings.get(word2)\n",
        "#                 if word2 != word:\n",
        "#                     corrected.append((word, word2))\n",
        "#             else:\n",
        "#                 words_not_found.append(word)\n",
        "#     return matrix, corrected, words_not_found"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "0a87e77d-78db-44b5-9358-d376f59d01a7",
        "_cell_guid": "2ee3bda0-ae09-4739-8321-29fb21b577a6",
        "trusted": true,
        "id": "h0xRfyx47ycd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('word_index.pickle', 'rb') as handle:\n",
        "    word_index = pickle.load(handle)\n",
        "\n",
        "with open('../input/mappings/ft_map.pickle', 'rb') as handle:\n",
        "    ft_map = pickle.load(handle)\n",
        "\n",
        "with open('../input/mappings/gl_map.pickle', 'rb') as handle:\n",
        "    gl_map = pickle.load(handle)\n",
        "\n",
        "print(len(ft_map), len(gl_map))\n",
        "\n",
        "nb_words = min(max_features, len(word_index))\n",
        "\n",
        "def build_matrix_1(path, nb_words, embed_size, correction_map):\n",
        "    embeddings = dict(get_coefs(*o.strip().split(' ')) for o in open(path))\n",
        "    embeddings_keys = list(embeddings.keys())\n",
        "    corrected = []\n",
        "    words_not_found = []\n",
        "    matrix = np.zeros((nb_words, embed_size))\n",
        "    \n",
        "    for word, i in tqdm(word_index.items()):\n",
        "        if i >= nb_words:\n",
        "            break\n",
        "        else:\n",
        "            if embeddings.get(word) is not None:\n",
        "                matrix[i] = embeddings.get(word)\n",
        "            elif correction_map.get(word) is not None:\n",
        "                matrix[i] = embeddings.get(correction_map.get(word))\n",
        "                corrected.append((word, correction_map.get(word)))\n",
        "            else:\n",
        "                words_not_found.append(word)\n",
        "    return matrix, corrected, words_not_found"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e2d40cd4-421f-423c-9fd4-d56cdd373abf",
        "_cell_guid": "e85ce35d-8f43-47c0-bde4-f81e5f6a29be",
        "trusted": true,
        "id": "dFKCH-L47ych",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_ft, corrected_ft, words_not_found_ft = build_matrix_1(ft_path, nb_words, 300, ft_map)\n",
        "embedding_gl, corrected_gl, words_not_found_gl = build_matrix_1(gl_path, nb_words, 300, gl_map)\n",
        "\n",
        "embedding_matrix = np.concatenate((embedding_ft, embedding_gl), axis=-1)\n",
        "\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "6f2f868d-6e3f-4c73-96bf-e4b3b9c4fbbb",
        "_cell_guid": "18c00746-527f-4c56-9879-62977935940a",
        "trusted": true,
        "id": "66REzfyu7yco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(corrected_ft), len(words_not_found_ft))\n",
        "\n",
        "print(corrected_ft)\n",
        "print(words_not_found_ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "a6ac54cd-9a83-4ca3-8ca9-2752c7f90c0c",
        "_cell_guid": "135cb3b9-8851-4213-80e4-1ac29a51b17d",
        "trusted": true,
        "id": "VLOC8LUK7ycs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(corrected_gl), len(words_not_found_gl))\n",
        "\n",
        "print(corrected_gl)\n",
        "print(words_not_found_gl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e3d146d8-be54-4758-bcbf-ab342285cc1a",
        "_cell_guid": "92e191dc-5cc7-45d5-8bf1-1d5705db2a01",
        "trusted": true,
        "id": "pJ0o2csQ7ycv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('embedding_matrix.npy', embedding_matrix)\n",
        "\n",
        "del embedding_matrix, words_not_found_ft, words_not_found_gl, corrected_ft, corrected_gl, embedding_ft, embedding_gl\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "239f1ddc-2b5c-4c93-9801-a741315b2615",
        "_cell_guid": "4bd23a12-9fc3-45b9-aa13-f8a1c1a1206a",
        "trusted": true,
        "id": "NQ91dQa67ycz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.load('x_train.npy')\n",
        "x_test = np.load('x_test.npy')\n",
        "y_train = np.load('y_train.npy')\n",
        "y_aux_train = np.load('y_aux_train.npy')\n",
        "weights = np.load('weights.npy')\n",
        "embedding_matrix = np.load('embedding_matrix.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "551994d6-16b1-4d07-a9a2-cf105890ea70",
        "_cell_guid": "b4b4b2dc-3eb0-4ef6-b0b6-0889722c8c97",
        "trusted": true,
        "id": "u3XT-T207yc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_MODELS = 1\n",
        "LSTM_UNITS = 128\n",
        "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
        "    \n",
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self, embedding_matrix, num_aux_targets):\n",
        "    super(NeuralNet, self).__init__()\n",
        "    embed_size = embedding_matrix.shape[1]\n",
        "\n",
        "    self.embedding = nn.Embedding(max_features, embed_size)\n",
        "    self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "    self.embedding.weight.requires_grad = False\n",
        "    self.embedding_dropout = nn.Dropout2d(0.3)\n",
        "\n",
        "    self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
        "    self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
        "\n",
        "    self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
        "    self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
        "\n",
        "    self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n",
        "    self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n",
        "        \n",
        "  def forward(self, x):\n",
        "    h_embedding = self.embedding(x)\n",
        "\n",
        "    embeddings = h_embedding.unsqueeze(2)    # (N, T, 1, K)\n",
        "    embeddings = embeddings.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
        "    embeddings = self.embedding_dropout(embeddings)  # (N, K, 1, T), some features are masked\n",
        "    embeddings = embeddings.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
        "    h_embedding = embeddings.squeeze(2)  # (N, T, K)\n",
        "\n",
        "    h_lstm1, _ = self.lstm1(h_embedding)\n",
        "    h_lstm2, _ = self.lstm2(h_lstm1)\n",
        "\n",
        "    avg_pool = torch.mean(h_lstm2, 1)\n",
        "    max_pool, _ = torch.max(h_lstm2, 1)\n",
        "\n",
        "    h_conc = torch.cat((max_pool, avg_pool), 1)\n",
        "    h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
        "    h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
        "\n",
        "    hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
        "\n",
        "    # sigmoid layer included within BCEWithLogitLoss\n",
        "    result = self.linear_out(hidden)\n",
        "    aux_result = self.linear_aux_out(hidden)\n",
        "    out = torch.cat([result, aux_result], 1)\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "c21d8529-7747-4675-b98a-dc1a6f28ad2e",
        "_cell_guid": "ecd48dc9-e709-4b28-b7dc-6463d54114cd",
        "trusted": true,
        "id": "fUGP6Qnx7yc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_loss(data, targets):\n",
        "    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:, 1:2])(data[:, :1], targets[:, :1])\n",
        "    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:, 1:2], targets[:, 2:3])\n",
        "    bce_loss_3 = nn.BCEWithLogitsLoss()(data[:, 2:3], targets[:, 3:4])\n",
        "    bce_loss_4 = nn.BCEWithLogitsLoss()(data[:, 3:4], targets[:, 4:5])\n",
        "    bce_loss_5 = nn.BCEWithLogitsLoss()(data[:, 4:5], targets[:, 5:6])\n",
        "    bce_loss_6 = nn.BCEWithLogitsLoss()(data[:, 5:6], targets[:, 6:7])\n",
        "    bce_loss_7 = nn.BCEWithLogitsLoss()(data[:, 6:7], targets[:, 7:8])\n",
        "    bce_loss_8 = nn.BCEWithLogitsLoss()(data[:, 7:8], targets[:, 8:9])\n",
        "\n",
        "    return bce_loss_1 + bce_loss_2 + bce_loss_3 + bce_loss_4 + bce_loss_5 + bce_loss_6 + bce_loss_7 + bce_loss_8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "ce0885f7-2b5f-4654-a683-cf5562b37ac3",
        "_cell_guid": "c6533232-54cb-4c9a-855e-e71d772e6b48",
        "trusted": true,
        "id": "w7C4IIH37ydC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_d = NeuralNet(embedding_matrix, len(y_aux_train))\n",
        "print(model_d)\n",
        "del model_d\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "2d440a0e-ddda-4795-be84-4582d71b5de0",
        "_cell_guid": "65a6f299-86bc-4c3e-989e-31dfcff431d0",
        "trusted": true,
        "id": "c4i2Rsr37ydH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def train_model(model_obj, x_train, y_train, x_test, seed, lr=0.001, batch_size=512, n_epochs=3, n_splits=5):\n",
        "  \n",
        "  output_dim = y_train.shape[-1]-1\n",
        "  x_test_torch = torch.tensor(x_test, dtype=torch.long).cuda()\n",
        "  test_loader = torch.utils.data.DataLoader(x_test_torch, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  avg_losses = []\n",
        "  avg_val_losses = []\n",
        "\n",
        "  train_preds = np.zeros((len(x_train)))\n",
        "  test_preds = np.zeros((len(x_test), output_dim))\n",
        "\n",
        "  splits = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed).split(x_train, y_train[:, 0]))\n",
        "  \n",
        "  for i, (train_idx, valid_idx) in enumerate(splits):\n",
        "    x_train_fold = torch.tensor(x_train[train_idx], dtype=torch.long).cuda()\n",
        "    y_train_fold = torch.tensor(y_train[train_idx], dtype=torch.float32).cuda()\n",
        "    x_val_fold = torch.tensor(x_train[valid_idx], dtype=torch.long).cuda()\n",
        "    y_val_fold = torch.tensor(y_train[valid_idx], dtype=torch.float32).cuda()\n",
        "      \n",
        "    model = copy.deepcopy(model_obj)\n",
        "    model.cuda()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n",
        "\n",
        "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
        "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
        "      \n",
        "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    print('Fold: ', i)\n",
        "    for epoch in range(n_epochs):\n",
        "      start_time = time.time()\n",
        "\n",
        "      model.train()\n",
        "      avg_loss = 0.\n",
        "\n",
        "      for x_batch, y_batch in tqdm(train_loader):\n",
        "        y_pred = model(x_batch)\n",
        "        loss = custom_loss(y_pred, y_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        avg_loss += loss.item() / len(train_loader)\n",
        "        \n",
        "      model.eval()\n",
        "      valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
        "      test_preds_fold = np.zeros((len(x_test), output_dim))\n",
        "      avg_val_loss = 0.\n",
        "\n",
        "      for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
        "        y_pred = model(x_batch).detach()\n",
        "        avg_val_loss += custom_loss(y_pred, y_batch).item() / len(valid_loader)\n",
        "        valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy()[:, 0])\n",
        "  \n",
        "      scheduler.step()\n",
        "      \n",
        "      elapsed_time = time.time() - start_time \n",
        "      print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n",
        "          epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n",
        "      \n",
        "    avg_losses.append(avg_loss)\n",
        "    avg_val_losses.append(avg_val_loss)\n",
        "    train_preds[valid_idx] = valid_preds_fold\n",
        "\n",
        "    for i, (x_batch) in enumerate(test_loader):\n",
        "      y_pred = sigmoid(model(x_batch).detach().cpu().numpy())\n",
        "      test_preds_fold[i * batch_size:(i+1) * batch_size] = y_pred\n",
        "          \n",
        "    test_preds += test_preds_fold / len(splits)\n",
        "    del model, optimizer, scheduler, loss\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  print('All \\t loss={:.4f} \\t val_loss={:.4f}'.format(np.average(avg_losses),np.average(avg_val_losses)))\n",
        "  return train_preds, test_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "b91fc4aa-1715-4d35-a261-ed0a95ea54c8",
        "_cell_guid": "b56a8876-4451-46e2-aba5-f9a6098bca02",
        "trusted": true,
        "id": "u04nu1617ydM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_everything(seed=1234):\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "y_train_combined = np.hstack([y_train.T[:, np.newaxis], weights.T[:, np.newaxis], y_aux_train])\n",
        "print(y_train_combined.shape)\n",
        "\n",
        "all_train_preds = []\n",
        "all_test_preds = []\n",
        "\n",
        "for model_idx in range(NUM_MODELS):\n",
        "  \n",
        "  print('Model ', model_idx)\n",
        "  SEED = 1234+model_idx\n",
        "  seed_everything(SEED)\n",
        "  model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n",
        "  \n",
        "  train_preds, test_preds = train_model(model, x_train, y_train_combined, x_test, seed=SEED)\n",
        "  \n",
        "  all_train_preds.append(train_preds)\n",
        "  all_test_preds.append(test_preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "037efaaf-fa62-469d-a64e-bba27b918668",
        "_cell_guid": "0a2ba94e-f9df-4686-a843-5a92b7716bc4",
        "trusted": true,
        "id": "A4QdJEyC7ydQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = pd.DataFrame({'id': test['id'], 'prediction': np.mean(all_test_preds, axis=0)[:, 0]})\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
